{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6d4318cfcf5ab38330ce9a2252c5604",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2025<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted, unless you have one of the excusable circumstances outlined in the syllabus (you or someone you need to care for are ill, you have a personal or family emergency, etc).\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. **Don't use editors *other* than Jupyter Notebook** to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: *\\<Riley Thompson\\>*\n",
    "\n",
    "people I talked to about the assignment: *\\<N/A\\>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3326bff2878b519d9a64325913cdfa8",
     "grade": false,
     "grade_id": "hw4header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #4\n",
    "\n",
    "**This is due Wednesday, November 12, 2025 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a011039658a7c8780f41fbca9f8d11ae",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import isclose\n",
    "\n",
    "# Used in the cosimfreq() implementation from class\n",
    "import numpy as np\n",
    "\n",
    "# HINT: This might make your life much easier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e94107167185f141e929cb6dcbf0f10",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "Again, for grading, I need to be working with the right file that we load our corpus from. On my machine, that file has this path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28497011f8bd6b55ec508bc5635b041a",
     "grade": false,
     "grade_id": "cell-b7e8e00e5c53b643",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a5b37d970ffefc8e8fb9ed52a2f6b7f",
     "grade": false,
     "grade_id": "cell-26968d9281db051c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "For **you** to work on your own code, you need to point this notebook to the path for this file on your own machine. *You should enter the path on your own machine in the editable code cell below,* then uncomment that line so the notebook works on your machine. This means that the second code cell will take precedence in assigning the value of the path to the corpus, and you can write your code to open that file without problems.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.** This means that when I run the code on my own machine, it'll have the path that ***I*** need, and it'll grade your notebook properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "#newbfile = '/home/rthompson/arizona/hlt531/NewB/train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c654124bea487036c0fd65511774128",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Build a frequency-based *document* index from the `train_orig.txt` file. (6 points total)\n",
    "\n",
    "You may adapt any of the code from class for this or use your own. You may also use or adapt the code from your previous assignments. Do *not* stem or remove stop words, but *do* use the processing that we've done before: leave only upper and lower case ASCII letters, the percent sign, and numbers. We'll continue the practice from last week of putting our text normalization into a separate function, `text_prep()`, so that we can easily do the same processing for our collection and for our queries.\n",
    "\n",
    "Your `makeFreqIdx()` function should return a list where the positional index of each item (ie, what you might get from `enumerate`-ing the document index) corresponds to the document ID. The value of each item in the list is a tuple composed of: i) the publication source code for that document, as an integer; ii) the text of that document, normalized and tokenized, as a list of strings; and iii) a set that contains tuples of terms and counts—a word (a normalized string) and a count (an integer) of how many times it occurs in this document.\n",
    "\n",
    "You should see that this is really quite similar to what you wrote last week. The only difference comes in the set that is the third element of each document in our collection. Last week, that set was composed of just terms. This week, that set is composed of tuples of terms and counts. Now that we're counting terms, do you think a `Counter()` object would help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97ea594fa93f5ab335a0a3538ddf0a1b",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# NOTE: This time, we're not returning both a normalized string AND\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m#       a tokenized list of strings--just the tokenized list of strings.\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[^a-zA-Z0-9\u001b[39m\u001b[33m%\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;28minput\u001b[39m).split()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m docs = \u001b[43mmakeFreqIdx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[32m250922\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmakeFreqIdx\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''create a frequency-based document index from\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mthe newB source file\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m            set of count tuples: (word, count)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     17\u001b[39m     lines = [line.rstrip(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[32m     19\u001b[39m docs = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arizona/hlt531/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'"
     ]
    }
   ],
   "source": [
    "def makeFreqIdx(filepath):\n",
    "    '''create a frequency-based document index from\n",
    "    the newB source file\n",
    "    \n",
    "    args:\n",
    "        filepath (str): location of train_orig.txt\n",
    "    returns:\n",
    "        documents (List[Tuple[int, List[str], Set[Tuple[str, int]]]]): \n",
    "        a frequency-based document index represented\n",
    "            as a list of tuples:\n",
    "                publication code, as an integer\n",
    "                tokenized text of document (ie, one sentence), as a list of strings\n",
    "                set of count tuples: (word, count)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.rstrip(\"\\n\") for line in file]\n",
    "\n",
    "    docs = []\n",
    "    for line in lines:\n",
    "        id = int(line[0])\n",
    "        text = text_prep(line[2:])\n",
    "        words = set(Counter(text).items())\n",
    "        \n",
    "        docs.append((id,text,words))\n",
    "    return docs\n",
    "\n",
    "        \n",
    "def text_prep(input):\n",
    "    '''performs text normalization and tokenization on an input string\n",
    "    \n",
    "    Our process: anything that is not a letter (upper or lower case\n",
    "        ASCII letters), digit (0-9), or the percent sign (%) is converted\n",
    "        to space, and then terms are split on whitespace.\n",
    "    \n",
    "    args:\n",
    "        input (str):        a string of unprocessed text\n",
    "    returns:\n",
    "        output (List[str]): a list of strings of normalized tokens\n",
    "    '''\n",
    "    # NOTE: This time, we're not returning both a normalized string AND\n",
    "    #       a tokenized list of strings--just the tokenized list of strings.\n",
    "    # YOUR CODE HERE\n",
    "    return re.sub(r\"[^a-zA-Z0-9%]\",\" \",input).split()\n",
    "\n",
    "docs = makeFreqIdx(newbfile)\n",
    "print(docs[250922])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49492ce6f35df0408c7ffdaa1946e6f1",
     "grade": false,
     "grade_id": "cell-14d9477f348b4416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Here's a bit of a test for your `text_prep()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7617881909076c9eeaa114da7d02f30",
     "grade": false,
     "grade_id": "cell-ea0aadb36a1b3934",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  ['this', 'is', 'a', 'weird', 'string']\n",
      " Yours:  ['this', 'is', 'a', 'weird', 'string']\n",
      "Result:  Matches! \n",
      "\n",
      "Target:  ['this', 'string', 'is', '98%', 'oDd']\n",
      " Yours:  ['this', 'string', 'is', '98%', 'oDd']\n",
      "Result:  Matches! \n",
      "\n",
      "Target:  ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
      " Yours:  ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
      "Result:  Matches! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some sample input\n",
    "test1 = 'this is a_weird string  \\n'\n",
    "test2 = 'this string is 98% oDd.'\n",
    "test3 = 'now we have some WĔIřD•characters'\n",
    "tests = [test1, test2, test3]\n",
    "\n",
    "#pay attention to how this should be tokenized according to the instructions\n",
    "target1 = ['this', 'is', 'a', 'weird', 'string']\n",
    "target2 = ['this', 'string', 'is', '98%', 'oDd']\n",
    "target3 = ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
    "targets = [target1, target2, target3]\n",
    "\n",
    "for test, target in zip(tests, targets):\n",
    "    yourout = text_prep(test)\n",
    "    result = \"Matches!\" if target == yourout else \"Doesn't match!\"\n",
    "    print(\"Target: \",target)\n",
    "    print(\" Yours: \",yourout)\n",
    "    print(\"Result: \", result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e88bbef6981cfc718c4925babc2547",
     "grade": false,
     "grade_id": "cell-b204489d35ff03fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'this': 1, 'string': 1, 'is': 1, '98%': 1, 'oDd': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of working with a Counter()\n",
    "count2 = Counter(target2)\n",
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07ba0aec542494ede4ca2aa9efa1e0a7",
     "grade": false,
     "grade_id": "cell-e770c8bbf6d64c2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('98%', 1), ('is', 1), ('oDd', 1), ('string', 1), ('this', 1)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a way to get the Counter() to give you a set of (key, value) tuples\n",
    "#  (the set() function is there just to get the types right)\n",
    "set(count2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdb6b36f0fe94d44e66c463b23fd0887",
     "grade": false,
     "grade_id": "cell-bc0564060a5ff503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how to get something from a set of tuples back into a Counter()\n",
    "mytuples = {('98%', 1), ('is', 1), ('oDd', 1), ('string', 1), ('this', 1)}\n",
    "\n",
    "# The trick is to convert this set of tuples into a dictionary, and from\n",
    "#   there easily into a Counter()\n",
    "newcounter = Counter(dict(mytuples))\n",
    "\n",
    "#Is this the same as what we started with?\n",
    "newcounter == count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "235e22215b71662f5fce01e983936567",
     "grade": false,
     "grade_id": "cell-20d8ab566b2d4c25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One of the nice things about `Counter()` objects is that they don't complain if you ask for a key that they don't have; they simply return a value of zero (ie, they didn't count that key when they were created). This can make for some Python code that is MUCH easier to read! Consider this example of a dot product function on `Counter()` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d1842756178242e14fc1304070c4bb",
     "grade": false,
     "grade_id": "cell-c3d90b5b77264c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cdot(counter1, counter2):\n",
    "    '''calculate the dot product over two counters'''\n",
    "    return sum(counter1[word]*counter2[word] for word in counter1)\n",
    "    # Think about why I only have to iterate over one of the counters for this calculation\n",
    "    # Could you also write a function to find the \"vector magnitude\" of a Counter()?\n",
    "    # With those two functions, could you also rewrite cosim() in terms of Counter()s?\n",
    "\n",
    "count1, count2 = Counter(target1), Counter(target2)\n",
    "cdot(count1, count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ced3fe123f1009a90a4aef334cbe94de",
     "grade": false,
     "grade_id": "cell-b0804ca5b161f088",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Tests for Q1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "968f87d8b4abaf9f7fd88230c65d5dfe",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#this will take a few seconds\n",
    "docs = makeFreqIdx(newbfile)\n",
    "\n",
    "# test 1a, 1 pt\n",
    "assert isinstance(docs, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9d1320cfb64ae7760710e1a00600a35",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test 1b, 1 pt\n",
    "assert len(docs) == 253781 and all(isinstance(doc,tuple) for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e6b50db1d6c08b5613087c12f0fbe6",
     "grade": true,
     "grade_id": "q1t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1c, 1 pt\n",
    "assert all(isinstance(doc[0],int) and isinstance(doc[1],list) and isinstance(doc[2],set) for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5800d06b12bb861f7bbd248ee24337cd",
     "grade": true,
     "grade_id": "q1t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test 1d, 1 pt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m docs[\u001b[32m250922\u001b[39m][\u001b[32m0\u001b[39m] == \u001b[32m10\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# test 1d, 1 pt\n",
    "assert docs[250922][0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9926ca397ac0ee01f3b88054f6fa556",
     "grade": true,
     "grade_id": "q1t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1e, 1 pt\n",
    "assert docs[10000][1] == ['the', 'snack', 'bar', 'across', 'the', 'mall',\n",
    "                          'from', 'the', 'trump', 'on', 'the', 'ocean',\n",
    "                          'site', 'was', 'badly', 'flooded', 'and', 'damaged',\n",
    "                          'even', 'though', 'it', 'has', 'no', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36d25afe905394c518112274e11eb328",
     "grade": true,
     "grade_id": "q1t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1f, 1 pt\n",
    "assert docs[38032][2] == {('a', 2), ('agreed', 1), ('and', 2), ('as', 2),\n",
    " ('asked', 1), ('be', 2), ('but', 1), ('by', 1), ('did', 1), ('even', 1),\n",
    " ('far', 1), ('fear', 1), ('fed', 3), ('from', 1), ('government', 1), ('im', 3),\n",
    " ('in', 1), ('is', 1), ('known', 1), ('lives', 1), ('lumber', 1), ('name', 1),\n",
    " ('named', 1), ('not', 2), ('of', 1), ('others', 1), ('partisan', 1), ('quoted', 1),\n",
    " ('rancor', 1), ('reflecting', 1), ('regulation', 1), ('said', 1), ('square', 1),\n",
    " ('stronghold', 1), ('taxes', 1), ('the', 1), ('then', 1), ('to', 2), ('trump', 2),\n",
    " ('up', 3), ('what', 1), ('who', 2), ('wholesaler', 1), ('with', 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfe38143ce88ada763af25d4d752aad3",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2**. Revise our Euclidean distance function to operate over these new index structures, now that we're using frequency counts. (4 points total)\n",
    "\n",
    "Note, this means that we need to adjust that function since we're not just looking at term incidence, where all weights effectively equal 1. Be careful; this can be tricky! There are different ways to approach it, and some are faster than others.\n",
    "\n",
    "*(Hint: What are the different cases of values in the two vectors that you have to calculate for this? How much of the code from the in-class notebook—both from last week's Euclidean distance function that worked on unweighted vectors, and this week's cosine similarity function that did work on weighted vectors—can you re-use? Here's one place where using a Counter() may give you some helpful default behavior. Pay attention to the TYPES that this function expects--you can't just pass it a Counter(), but you might be able to convert a set of tuples into a Counter() pretty easily, right?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "771560946daae6d00446ea1159575f78",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def eucdistfreq(d1,d2):\n",
    "    '''calculate Euclidean distance for frequency-based\n",
    "    document representations as produced by makeFreqIdx()\n",
    "    \n",
    "    args:\n",
    "        d1,d2 (each one as set[Tuple[str, int]]):\n",
    "            document vectors represented as sets of\n",
    "            tuples of the form (word,count)\n",
    "    returns:\n",
    "        (float) Euclidean distance between the two document vectors\n",
    "    '''\n",
    "    freq1 = dict(d1)\n",
    "    freq2 = dict(d2)\n",
    "    \n",
    "    all_words = set(freq1.keys()).union(freq2.keys())\n",
    "    \n",
    "    dist_sq = 0\n",
    "    for w in all_words:\n",
    "        c1 = freq1.get(w, 0)\n",
    "        c2 = freq2.get(w, 0)\n",
    "        dist_sq += (c1 - c2) ** 2\n",
    "    \n",
    "    return np.sqrt(dist_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1164ade14e18a63dca1125d036d6b6c",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's get two documents to use\n",
    "d0 = docs[0][2]\n",
    "d8 = docs[8][2]\n",
    "\n",
    "# The order of these vectors shouldn't matter\n",
    "res1 = eucdistfreq(d0,d8)\n",
    "res2 = eucdistfreq(d8,d0)\n",
    "\n",
    "# test 2a, 1pt\n",
    "assert isinstance(res1, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58c7f6541bbe0ca527efaafcfbf59d64",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1pt\n",
    "# The order that the vectors are fed to eucdistfreq shouldn't change the result\n",
    "assert res1 == res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214f0313686f1c7bb2bb93c0f374e136",
     "grade": true,
     "grade_id": "q2t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2c, 1pt\n",
    "# I get a value 5.477225575051661\n",
    "assert isclose(res1,5.4772,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a9e84a7b87ebd8d2e9bf841a601f60",
     "grade": true,
     "grade_id": "q2t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d2 = docs[2][2]\n",
    "res3 = eucdistfreq(d8,d2)\n",
    "\n",
    "# test 2d, 1pt\n",
    "# I get a value 7.0710678118654755\n",
    "assert isclose(res3,7.0711,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "702ed1fbbc27ceec96b3d5b70e87884c",
     "grade": false,
     "grade_id": "fromclass1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For what's coming below, we'll also need the `cosimfreq()` function from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6501c3460e863096564893f67f9859e1",
     "grade": false,
     "grade_id": "fromclass2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#cosine similarity wrt/frequencies for DTI (from class)\n",
    "def cosimfreq(d1,d2):\n",
    "    num = sum([e1[1]*e2[1] for e1 in d1\n",
    "               for e2 in d2 if e1[0] == e2[0]])\n",
    "    d1len = np.sqrt(sum([e[1]**2 for e in d1]))\n",
    "    d2len = np.sqrt(sum([e[1]**2 for e in d2]))\n",
    "    denom = d1len * d2len\n",
    "    if denom == 0: return 0\n",
    "    return float(num)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54235c0b79f384ecbb193acd394a4e7a",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Revise your search function from last time so that it works with weighted document vectors and query vectors. (5 points)\n",
    "\n",
    "**It should process the query just like you processed your documents,** and then return the top 10 document indices that best match a query using either Euclidean distance or cosine similarity. (Remember that sorting should depend on the distance metric that is used.)\n",
    "\n",
    "The function should have this argument structure:\n",
    "\n",
    "```python\n",
    "search(query,index,cosine=True)\n",
    "```\n",
    "\n",
    "The default is cosine similarity, but if you specify a third argument as `False`, the function uses Euclidean distance. (You may, of course, use and adapt code from class.)\n",
    "\n",
    "*Hint: This week, we've changed our index, and we've changed our distance functions. What part of your search function from last week needs to change, to be compatible with these differences?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a6b73b43e137ef4a7142f5883e70eb8",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def search(query,index,cosine=True):\n",
    "    '''searches for the 10 best matches for a\n",
    "    string query using either cosine similarity\n",
    "    or euclidean distance\n",
    "    \n",
    "    args:\n",
    "        q (str): a query, possibly multi-term; we will assume that\n",
    "            we'll need to split it on whitespace and normalize, just\n",
    "            like we did for our document collection\n",
    "        idx (List[...]): frequency-based index, from makeFreqIndex()\n",
    "        cosine (boolean): for selecting cosine similarity or\n",
    "            euclidean distance as the similarity measure\n",
    "    returns:\n",
    "        best10 (List[Tuple[float, int]])\n",
    "        list of the 10 best matching documents as (score,docId)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    q_tokens = text_prep(query)\n",
    "    \n",
    "    q_freq = set((w, q_tokens.count(w)) for w in set(q_tokens))\n",
    "    \n",
    "    func = cosimfreq if cosine else eucdistfreq\n",
    "    \n",
    "    scores = []\n",
    "    for doc_id, doc in enumerate(index):\n",
    "        score = func(doc[2], q_freq)\n",
    "        scores.append((score, doc_id))\n",
    "    \n",
    "    scores.sort(reverse=cosine, key=lambda x: x[0])\n",
    "    \n",
    "    return scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13509782996a90c5aeef74231a594110",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r1 = search('the fire wall',docs)\n",
    "\n",
    "# test 3a, 1pt\n",
    "assert isinstance(r1, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46f3b017ddf6e3d4176ad840323bfaa5",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1pt\n",
    "assert len(r1) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31656aa4839bbc4818d358e4a8c8912a",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r2 = search('the fire wall',docs,False)\n",
    "\n",
    "# test 3c, 1pt\n",
    "assert r1[0][1] == 23855 and r2[0][1] == 22610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc45af6793a78d4f42d18734dd313de3",
     "grade": false,
     "grade_id": "cell-2e7312539bcc7858",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosimfreq top 10 results:\teucdistfreq top 10 results:\n",
      "(0.5773502691896258, 23855)\t(np.float64(1.7320508075688772), 22610)\n",
      "(0.5560384374855327, 101886)\t(np.float64(1.7320508075688772), 23855)\n",
      "(0.545544725589981, 20432)\t(np.float64(2.0), 2014)\n",
      "(0.5345224838248488, 217696)\t(np.float64(2.0), 3034)\n",
      "(0.5333333333333333, 132093)\t(np.float64(2.0), 3950)\n",
      "(0.5270462766947299, 113628)\t(np.float64(2.0), 4009)\n",
      "(0.5222329678670935, 74900)\t(np.float64(2.0), 4280)\n",
      "(0.5222329678670935, 103169)\t(np.float64(2.0), 4350)\n",
      "(0.5222329678670935, 182303)\t(np.float64(2.0), 4524)\n",
      "(0.5163977794943222, 155389)\t(np.float64(2.0), 4830)\n"
     ]
    }
   ],
   "source": [
    "# Let's display these\n",
    "print('cosimfreq top 10 results:\\teucdistfreq top 10 results:')\n",
    "for r1e, r2e in zip(r1, r2):\n",
    "    print(f'{r1e}\\t{r2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b424aa40929fa85cac170fe1a689f491",
     "grade": true,
     "grade_id": "q3t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3d, 1pt\n",
    "# I get 0.5773502691896258\n",
    "assert isclose(r1[0][0],.5774,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a83c4f4491d243b080d590ade3b5d3",
     "grade": true,
     "grade_id": "q3t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3e, 1pt\n",
    "# I get 1.7320508075688772\n",
    "assert isclose(r2[0][0],1.7321,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1634608a7a41b50b142cd8a03ed624b3",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** This system should have problems related to function words (since we are not removing them). Explain what the problem is. (2 pts)\n",
    "\n",
    "*For full points, be sure your answer makes it clear you understand how the system we've been building **in this assignment** may return results that do not actually meet a user's information need well, as a consequence of function words in the collection and in the query.*\n",
    "\n",
    "*Hint: This problem was discussed in the course videos, so if you're not sure how to approach this, you might benefit from reviewing the videos. For what you write here, you should be able to express this problem in your own words.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a029c7de09ed9485528a3d2cf6a9644",
     "grade": true,
     "grade_id": "q4a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The problem is that function words will appear in pretty much every document. If the qurey has the word \"the\" in it, then the results will be skewed because the majority of documents are likely to have the word \"the\" or similar function words. We don't do any kind of TF-IDF ranking these words aren't regulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e98c9c6705c1c4ea6decbc21f6aa84e7",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** *Demonstrate* the problem that you described in **Question 4** with actual searches on the index and search function you created in questions 1 and 3. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392a5985755976aaa11468a304427cd7",
     "grade": true,
     "grade_id": "q5a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump', 'has', 'the', 'fire']\n",
      "['2016', 'the', 'wall', 'would', 'be', 'built', 'with', 'the', 'understanding', 'that', 'the', 'country', 'of', 'mexico', 'will', 'be', 'reimbursing', 'the', 'united', 'states', 'for', 'the', 'full', 'cost', 'of', 'such', 'wall', 'said', 'the', 'document', 'which', 'trump', 'signed']\n",
      "['when', 'he', 'said', 'donald', 'trump', 'will', 'build', 'the', 'wall', 'the', 'crowd', 'responded', 'with', 'chants', 'of', 'build', 'the', 'wall']\n",
      "['the', 'fire', 'in', 'california', 'where', 'i', 'was', 'if', 'you', 'looked', 'at', 'the', 'floor', 'the', 'floor', 'of', 'the', 'fire', 'they', 'have', 'trees', 'that', 'were', 'fallen', 'trump', 'said']\n",
      "['not', 'at', 'least', 'in', 'the', 'way', 'that', 'trump', 'envisions', 'it', 'a', 'massive', 'concrete', 'edifice', 'stretching', 'nearly', '2000', 'miles', 'from', 'the', 'gulf', 'of', 'mexico', 'to', 'the', 'pacific', 'ocean', 'a', 'wall', 'that', 's', 'more', 'impressive', 'than', 'the', 'great', 'wall', 'of', 'china', 'or', 'the', 'berlin', 'wall']\n",
      "['president', 'trump', 'should', 'understand', 'there', 'are', 'not', 'the', 'votes', 'for', 'the', 'wall', 'in', 'the', 'house', 'or', 'the', 'senate']\n",
      "['to', 'build', 'a', 'wall', 'mimicking', 'the', 'donald', 'trump', 'wall']\n",
      "['why', 'because', 'someday', 'theyll', 'name', 'the', 'wall', 'trump', 'wall']\n",
      "['from', 'the', 'right', 'does', 'trump', 'really', 'want', 'the', 'wall']\n",
      "['trump', 'i', 'dont', 'have', 'to', 'because', 'the', 'wall', 'is', 'the', 'same', 'wall', 'ive', 'always', 'talked', 'about']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\"\"\"You can see in the results of this search that documents that have a high frequency of the word \"the\" get prioritized even though it doesn't\n",
    "have anything to do really with the query. The presence of the words fire and wall in the docs help but the results are skewed. This could be fixed\n",
    "by removing stop words or with TF-IDF ranking.\"\"\"\n",
    "r1 = search('the fire wall',docs)\n",
    "for score in r1: print(docs[score[1]][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
